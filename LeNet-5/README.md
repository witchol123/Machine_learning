### 1. LeNet-5模型表达式

LeNet-5是卷积神经网络模型的早期代表，它由LeCun在1998年提出。该模型采用顺序结构，主要包括7层（2个卷积层、2个池化层和3个全连接层），卷积层和池化层交替排列。以mnist手写数字分类为例构建一个LeNet-5模型。每个手写数字图片样本的宽与高均为28像素，样本标签值是0~9，代表0至9十个数字。

![](https://ai-studio-static-online.cdn.bcebos.com/c758063e28754e20ac3ec70cef5ca1b0168ad923000d47f1bd686b59d2f3c23b)

图1. 单样本视角的LeNet-5模型原理

下面详细解析LeNet-5模型的正向传播过程。

（1）卷积层L1

单样本视角。L1层的输入数据形状大小为$\mathbb{R}^{1 \times 28 \times 28}$，表示通道数量为1，行与列的大小都为28。输出数据形状大小为$\mathbb{R}^{6 \times 24 \times 24}$，表示通道数量为6，行与列维都为24。

批量样本视角。设批量大小为m。L1层的输入数据形状大小为$\mathbb{R}^{m \times 1 \times 28 \times 28}$，表示样本批量为m，通道数量为1，行与列的大小都为28。L1层的输出数据形状大小为$\mathbb{R}^{m \times 6 \times 24 \times 24}$，表示样本批量为m，通道数量为6，行与列维都为24。

参数视角。L1层的权重形状大小$\mathbb{R}^{6 \times 1 \times 5 \times 5}$为，偏置项形状大小为6。

这里有两个问题很关键：一是，为什么通道数从1变成了6呢？原因是模型的卷积层L1设定了6个卷积核，每个卷积核都与输入数据发生运算，最终分别得到6组数据。二是，为什么行列大小从28变成了24呢？原因是每个卷积核的行维与列维都为5，卷积核（5×5）在输入数据（28×28）上移动，且每次移动步长为1，那么输出数据的行列大小分别为28-5+1=24。

（2）池化层L2

从单样本视角。L2层的输入数据大小要和L1层的输出数据大小保持一致。输入数据形状大小为$\mathbb{R}^{6 \times 24 \times 24}$，表示通道数量为6，行与列的大小都为24。L2层的输出数据形状大小为$\mathbb{R}^{6 \times 12 \times 12}$，表示通道数量为6，行与列维都为12。

从批量样本视角。设批量大小为m。L2层的输入数据形状大小为$\mathbb{R}^{m \times 6 \times 24 \times 24}$，表示样本批量为m，通道数量为6，行与列的大小都为24。L2层的输出数据形状大小为$\mathbb{R}^{m \times 6 \times 12 \times 12}$，表示样本批量为m，通道数量为6，行与列维都为12。为什么行列大小从24变成了12呢？原因是池化层中的过滤器形状大小为2×2，其在输入数据（24×24）上移动，且每次移动步长（跨距）为2，每次选择4个数（2×2）中最大值作为输出，那么输出数据的行列大小分别为24÷2=12。

（3）卷积层L3

单样本视角。L3层的输入数据形状大小为$\mathbb{R}^{6 \times 12 \times 12}$，表示通道数量为6，行与列的大小都为12。L3层的输出数据形状大小为$\mathbb{R}^{6 \times 8 \times 8}$，表示通道数量为16，行与列维都为8。

批量样本视角。设批量大小为m。L3层的输入数据形状大小为$\mathbb{R}^{m \times 6 \times 12 \times 12}$，表示样本批量为m，通道数量为6，行与列的大小都为12。L3层的输出数据形状大小为$\mathbb{R}^{m \times 16 \times 8 \times 8}$，表示样本批量为m，通道数量为16，行与列维都为8。

参数视角。L3层的权重形状大小为$\mathbb{R}^{m \times 16 \times 6 \times 5 \times 5}$，偏置项形状大小为16。

（4）池化层L4

从单样本视角。L4层的输入数据形状大小与L3层的输出数据大小一致。L4层的输入数据形状大小为$\mathbb{R}^{16 \times 8 \times 8}$，表示通道数量为16，行与列的大小都为8。L4层的输出数据形状大小为$\mathbb{R}^{16 \times 4 \times 4}$，表示通道数量为16，行与列维都为4。

从批量样本视角。设批量大小为m。L4层的输入数据形状大小为$\mathbb{R}^{m \times 16 \times 8 \times 8}$，表示样本批量为m，通道数量为16，行与列的大小都为8。L4层的输出数据形状大小为$\mathbb{R}^{m \times 16 \times 4 \times 4}$，表示样本批量为m，通道数量为16，行与列维都为4。池化层L4中的过滤器形状大小为2×2，其在输入数据（形状大小24×24）上移动，且每次移动步长（跨距）为2，每次选择4个数（形状大小2×2）中最大值作为输出。

（5）线性层L5

从单样本视角。由于L5层是线性层，其输入大小为一维，所以需要把L4层的输出数据大小进行重新划分。L4层的输出形状大小为$\mathbb{R}^{16 \times 4 \times 4}$，则L5层的一维输入形状大小为16×4×4=256。L4层的一维输出大小为120。

从批量样本视角。设批量大小为m。L5层输入数据形状大小为$\mathbb{R}^{m \times 256}$，表示样本批量为m，输入特征数量为256。输出数据形状大小为$\mathbb{R}^{m \times 120}$，表示样本批量为m，输出特征数量为120。

（6）线性层L6

从单样本视角。L6层的输入特征数量为120。L6层的输出特征数量为84。

从批量样本视角。设批量大小为m。L6层的输入数据形状大小为$\mathbb{R}^{m \times 120}$，表示样本批量为m，输入特征数量为120。L6层的输出数据形状大小为$\mathbb{R}^{m \times 84}$，表示样本批量为m，输出特征数量为84。

（7）线性层L7

从单样本视角。L7层的输入特征数量为84。L7层的输出特征数量为10。

从批量样本视角。设批量大小为m。L7层的输入数据形状大小为$\mathbb{R}^{m \times 84}$，表示样本批量为m，输入特征数量为84。L7层的输出数据形状大小为$\mathbb{R}^{m \times 10}$，表示样本批量为m，输出特征数量为10。

由于是分类问题，我们选择交叉熵损失函数。交叉熵主要用于衡量估计值与真实值之间的差距。交叉熵值越小，模型预测效果越好。

$E(\mathbf{y}^{i},\mathbf{\hat{y}}^{i})=-\sum_{j=1}^{q}\mathbf{y}_{j}^{i}ln(\mathbf{\hat{y}}_{j}^{i})$

其中，$\mathbf{y}^{i} \in \mathbb{R}^{q}$为真实值，$y_{j}^{i}$是$\mathbf{y}^{i}$中的元素(取值为0或1)，$j=1,...,q$。$\mathbf{\hat{y}^{i}} \in \mathbb{R}^{q}$是预测值（样本在每个类别上的概率）。

定义好了正向传播过程之后，接着随机化初始参数，然后便可以计算出每层的结果，每次将得到m×10的矩阵作为预测结果，其中m是小批量样本数。接下来进行反向传播过程，预测结果与真实结果之间肯定存在差异，以缩减该差异作为目标，计算模型参数梯度。进行多轮迭代，便可以优化模型，使得预测结果与真实结果之间更加接近。
